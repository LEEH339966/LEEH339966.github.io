{"title":"bert-utils和其他语句相似度计算的方法","slug":"笔记语句相似度的计算方法","date":"2020-12-27T12:04:58.000Z","updated":"2020-12-27T12:28:39.572Z","comments":true,"path":"api/articles/笔记语句相似度的计算方法.json","photos":[],"link":"","excerpt":"bert-utils和其他语句相似度计算的方法Bert模型的介绍BERT的全称为Bidirectional Encoder Representation from Transformers，是一个预训练的语言表征模型。它强调了不再像以往一样采用传统的单向语言模型或者把两个单向语言模型进行浅层拼接的方法进行预训练，而是采用新的masked language model（MLM），以致能生成深度的双向语言表征。BERT论文发表时提及在11个NLP（Natural Language Processing，自然语言处理）任务中获得了新的state-of-the-art的结果，令人目瞪口呆。该模型有以下主要优点：1）采用MLM对双向的Transformers进行预训练，以生成深层的双向语言表征。2）预训练后，只需要添加一个额外的输出层进行fine-tune，就可以在各种各样的下游任务中取得state-of-the-art的表现。在这过程中并不需要对BERT进行任务特定的结构修改。","covers":["https://leehbucket1.oss-cn-beijing.aliyuncs.com/img/9260af14410452b2bdf645b1cd80e632.jpg","https://pic1.zhimg.com/80/v2-2f4e0469fbb15ab3791888e5eba66cb4_1440w.jpg","https://pic1.zhimg.com/80/v2-3206310dd24e189a0580f8ab38e07424_1440w.jpg","https://pic3.zhimg.com/80/v2-f0618dc2c2f62bd8d71c2195947be1d6_1440w.jpg","https://pic3.zhimg.com/80/v2-102728d6cf40fb22febd01d63dd1d7da_720w.jpg","https://pic2.zhimg.com/80/v2-3d2a72e246eb12cf605726ce7b6fbf59_720w.jpg","https://pic1.zhimg.com/80/v2-a12ee6f717cc8312c43d140eb173def8_720w.jpg","https://pic1.zhimg.com/80/v2-ee823df66560850baa34128af76a6334_720w.jpg","https://leehbucket1.oss-cn-beijing.aliyuncs.com/img/v2-7e0666db23ec2c29358cc89e2f823a06_720w.png","https://www.zhihu.com/equation?tex=T_i","https://www.zhihu.com/equation?tex=T_i","https://leehbucket1.oss-cn-beijing.aliyuncs.com/img/v2-c8167e6b04726abe4421667abd027c3e_720w.jpg"],"content":"<hr>\n<h2 id=\"bert-utils和其他语句相似度计算的方法\"><a href=\"#bert-utils和其他语句相似度计算的方法\" class=\"headerlink\" title=\"bert-utils和其他语句相似度计算的方法\"></a>bert-utils和其他语句相似度计算的方法</h2><h4 id=\"Bert模型的介绍\"><a href=\"#Bert模型的介绍\" class=\"headerlink\" title=\"Bert模型的介绍\"></a>Bert模型的介绍</h4><p>BERT的全称为Bidirectional Encoder Representation from Transformers，是一个预训练的语言表征模型。它强调了不再像以往一样采用传统的单向语言模型或者把两个单向语言模型进行浅层拼接的方法进行预训练，而是采用新的<strong>masked language model（MLM）</strong>，以致能生成<strong>深度的双向</strong>语言表征。BERT论文发表时提及在11个NLP（Natural Language Processing，自然语言处理）任务中获得了新的state-of-the-art的结果，令人目瞪口呆。</p>\n<p>该模型有以下主要优点：</p>\n<p>1）采用MLM对双向的Transformers进行预训练，以生成深层的双向语言表征。</p>\n<p>2）预训练后，只需要添加一个额外的输出层进行fine-tune，就可以在各种各样的下游任务中取得state-of-the-art的表现。在这过程中并不需要对BERT进行任务特定的结构修改。</p>\n<a id=\"more\"></a>\n\n<p><img src=\"https://leehbucket1.oss-cn-beijing.aliyuncs.com/img/9260af14410452b2bdf645b1cd80e632.jpg\" alt=\"高清版 晚上 竹排 女孩 渔船 灯 好看二次元动漫壁纸\"></p>\n<h2 id=\"1-BERT的结构\"><a href=\"#1-BERT的结构\" class=\"headerlink\" title=\"1. BERT的结构\"></a>1. BERT的结构</h2><p>以往的预训练模型的结构会受到单向语言模型<em>（从左到右或者从右到左）</em>的限制，因而也限制了模型的表征能力，使其只能获取单方向的上下文信息。而BERT利用MLM进行预训练并且采用深层的双向Transformer组件<em>（单向的Transformer一般被称为Transformer decoder，其每一个token（符号）只会attend到目前往左的token。而双向的Transformer则被称为Transformer encoder，其每一个token会attend到所有的token。）</em>来构建整个模型，因此最终生成<strong>能融合左右上下文信息</strong>的深层双向语言表征。关于Transformer的详细解释可以参见*<em><a href=\"https://link.zhihu.com/?target=https://arxiv.org/abs/1706.03762\">Attention Is All You Need</a>** 或者 <strong><a href=\"https://link.zhihu.com/?target=https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></strong> **</em>（首推！）***。</p>\n<p>当隐藏了Transformer的详细结构后，我们就可以用一个只有输入和输出的黑盒子来表示它了：</p>\n<p><img src=\"https://pic1.zhimg.com/80/v2-2f4e0469fbb15ab3791888e5eba66cb4_1440w.jpg\" alt=\"img\"></p>\n<p>黑盒子Transformer而Transformer又可以进行堆叠，形成一个更深的神经网络：</p>\n<p><img src=\"https://pic1.zhimg.com/80/v2-3206310dd24e189a0580f8ab38e07424_1440w.jpg\" alt=\"img\"></p>\n<p>对Transformers进行堆叠最终，经过多层Transformer结构的堆叠后，形成BERT的主体结构：</p>\n<p><img src=\"https://pic3.zhimg.com/80/v2-f0618dc2c2f62bd8d71c2195947be1d6_1440w.jpg\" alt=\"img\">BERT的主体结构</p>\n<p><img src=\"https://pic3.zhimg.com/80/v2-102728d6cf40fb22febd01d63dd1d7da_720w.jpg\" alt=\"img\"></p>\n<p>该部分就是由多个Transformers所堆叠在一起对于不同的下游任务，BERT的结构可能会有不同的轻微变化，因此接下来只介绍<strong>预训练阶段</strong>的模型结构。</p>\n<h2 id=\"1-1-BERT的输入\"><a href=\"#1-1-BERT的输入\" class=\"headerlink\" title=\"1.1 BERT的输入\"></a><strong>1.1 BERT的输入</strong></h2><p><img src=\"https://pic2.zhimg.com/80/v2-3d2a72e246eb12cf605726ce7b6fbf59_720w.jpg\" alt=\"img\"></p>\n<p>BERT的输入为每一个token对应的表征<em>（图中的粉红色块就是token，黄色块就是token对应的表征）</em>，并且单词字典是采用WordPiece算法来进行构建的。为了完成具体的分类任务，除了单词的token之外，作者还在输入的每一个序列开头都插入特定的<strong>分类token（[CLS]）</strong>，该分类token对应的最后一个Transformer层输出被用来起到聚集整个序列表征信息的作用。</p>\n<p>由于BERT是一个预训练模型，其必须要适应各种各样的自然语言任务，因此模型所输入的序列必须有能力包含一句话<em>（文本情感分类，序列标注任务）</em>或者两句话以上<em>（文本摘要，自然语言推断，问答任务）</em>。那么如何令模型有能力去分辨哪个范围是属于句子A，哪个范围是属于句子B呢？BERT采用了两种方法去解决：</p>\n<p>1）在序列tokens中把<strong>分割token（[SEP]）</strong>插入到每个句子后，以分开不同的句子tokens。</p>\n<p>2）为每一个token表征都添加一个可学习的分割embedding来指示其属于句子A还是句子B。</p>\n<p>因此最后模型的输入序列tokens为下图<em>（如果输入序列只包含一个句子的话，则没有[SEP]及之后的token）</em>：</p>\n<p><img src=\"https://pic1.zhimg.com/80/v2-a12ee6f717cc8312c43d140eb173def8_720w.jpg\" alt=\"img\">模型的输入序列</p>\n<p>上面提到了BERT的输入为每一个token对应的表征，实际上该表征是由三部分组成的，分别是对应的<strong>token</strong>，<strong>分割</strong>和<strong>位置</strong> embeddings<em>（位置embeddings的详细解释可参见<a href=\"https://link.zhihu.com/?target=https://arxiv.org/abs/1706.03762\">Attention Is All You Need</a> 或 <a href=\"https://link.zhihu.com/?target=https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a>）</em>，如下图：</p>\n<p><img src=\"https://pic1.zhimg.com/80/v2-ee823df66560850baa34128af76a6334_720w.jpg\" alt=\"img\">token表征的组成</p>\n<p>到此为止，BERT的输入已经介绍完毕，可以看到其设计的思路十分简洁而且有效。</p>\n<h2 id=\"1-2-BERT的输出\"><a href=\"#1-2-BERT的输出\" class=\"headerlink\" title=\"1.2 BERT的输出\"></a>1.2 BERT的输出</h2><p>介绍完BERT的输入，实际上BERT的输出也就呼之欲出了，因为Transformer的特点就是有多少个输入就有多少个对应的输出，如下图：</p>\n<p><img src=\"https://leehbucket1.oss-cn-beijing.aliyuncs.com/img/v2-7e0666db23ec2c29358cc89e2f823a06_720w.png\" alt=\"img\">BERT的输出</p>\n<p><strong>C</strong>为分类token（[CLS]）对应最后一个Transformer的输出，<img src=\"https://www.zhihu.com/equation?tex=T_i\" alt=\"[公式]\"> 则代表其他token对应最后一个Transformer的输出。对于一些token级别的任务<em>（如，序列标注和问答任务）</em>，就把<img src=\"https://www.zhihu.com/equation?tex=T_i\" alt=\"[公式]\"> 输入到额外的输出层中进行预测。对于一些句子级别的任务<em>（如，自然语言推断和情感分类任务）</em>，就把<strong>C</strong>输入到额外的输出层中，这里也就解释了为什么要在每一个token序列前都要插入特定的分类token。</p>\n<p>到此为止，BERT的输入输出都已经介绍完毕了，更多具体的细节可以到原论文中察看。</p>\n<h2 id=\"2-BERT的预训练任务\"><a href=\"#2-BERT的预训练任务\" class=\"headerlink\" title=\"2. BERT的预训练任务\"></a>2. BERT的预训练任务</h2><p>实际上预训练的概念在CV（Computer Vision，计算机视觉）中已经是很成熟了，应用十分广泛。CV中所采用的预训练任务一般是ImageNet图像分类任务，完成图像分类任务的<strong>前提</strong>是必须能抽取出良好的图像特征，同时ImageNet数据集有规模大、质量高的优点，因此常常能够获得很好的效果。</p>\n<p>虽然NLP领域没有像ImageNet这样质量高的人工标注数据，但是可以利用大规模文本数据的<strong>自监督性质</strong>来构建预训练任务。因此BERT构建了两个预训练任务，分别是<strong>Masked Language Model</strong>和<strong>Next Sentence Prediction</strong>。</p>\n<h2 id=\"2-1-Masked-Language-Model（MLM）\"><a href=\"#2-1-Masked-Language-Model（MLM）\" class=\"headerlink\" title=\"2.1 Masked Language Model（MLM）\"></a>2.1 Masked Language Model（MLM）</h2><p>MLM是BERT能够不受单向语言模型所限制的原因。简单来说就是以15%的概率用mask token （[MASK]）随机地对每一个训练序列中的token进行替换，然后预测出[MASK]位置原有的单词。然而，由于[MASK]并不会出现在下游任务的微调（fine-tuning）阶段，因此预训练阶段和微调阶段之间产生了*<em>不匹配**</em>（这里很好解释，就是预训练的目标会令产生的语言表征对[MASK]敏感，但是却对其他token不敏感）*。因此BERT采用了以下策略来解决这个问题：</p>\n<p>首先在每一个训练序列中以15%的概率随机地选中某个token位置用于预测，假如是第i个token被选中，则会被替换成以下三个token之一</p>\n<p>1）80%的时候是[MASK]。如，my dog is <strong>hairy</strong>——&gt;my dog is <strong>[MASK]</strong></p>\n<p>2）10%的时候是随机的其他token。如，my dog is <strong>hairy</strong>——&gt;my dog is <strong>apple</strong></p>\n<p>3）10%的时候是原来的token<em>（保持不变，个人认为是作为2）所对应的负类）</em>。如，my dog is <strong>hairy</strong>——&gt;my dog is <strong>hairy</strong></p>\n<p>再用该位置对应的 <img src=\"https://www.zhihu.com/equation?tex=T_i\" alt=\"[公式]\"> 去预测出原来的token（<em>输入到全连接，然后用softmax输出每个token的概率，最后用交叉熵计算loss）</em>。</p>\n<p>该策略令到BERT不再只对[MASK]敏感，而是对所有的token都敏感，以致能抽取出任何token的表征信息。这里给出论文中关于该策略的实验数据：</p>\n<p><img src=\"https://leehbucket1.oss-cn-beijing.aliyuncs.com/img/v2-c8167e6b04726abe4421667abd027c3e_720w.jpg\" alt=\"img\">多种策略的实验结果</p>\n<h2 id=\"2-2-Next-Sentence-Prediction（NSP）\"><a href=\"#2-2-Next-Sentence-Prediction（NSP）\" class=\"headerlink\" title=\"2.2 Next Sentence Prediction（NSP）\"></a>2.2 Next Sentence Prediction（NSP）</h2><p>一些如问答、自然语言推断等任务需要理解两个句子之间的关系，而MLM任务倾向于抽取<strong>token层次</strong>的表征，因此不能直接获取<strong>句子层次</strong>的表征。为了使模型能够有能力理解句子间的关系，BERT使用了NSP任务来预训练，简单来说就是预测两个句子是否连在一起。具体的做法是：对于每一个训练样例，我们在语料库中挑选出句子A和句子B来组成，50%的时候句子B就是句子A的下一句<em>（标注为IsNext）</em>，剩下50%的时候句子B是语料库中的随机句子<em>（标注为NotNext）</em>。接下来把训练样例输入到BERT模型中，用[CLS]对应的C信息去进行二分类的预测。</p>\n<h2 id=\"2-3-预训练任务总结\"><a href=\"#2-3-预训练任务总结\" class=\"headerlink\" title=\"2.3 预训练任务总结\"></a>2.3 预训练任务总结</h2><p>最后训练样例长这样：</p>\n<p>Input1=[CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]</p>\n<p>Label1=IsNext</p>\n<p>Input2=[CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP]</p>\n<p>Label2=NotNext</p>\n<p>把每一个训练样例输入到BERT中可以相应获得两个任务对应的loss，再把这两个loss加在一起就是整体的预训练loss。<em>（也就是两个任务<strong>同时</strong>进行训练）</em></p>\n<p>可以明显地看出，这两个任务所需的数据其实都可以从<strong>无标签的</strong>文本数据中构建（自监督性质），比CV中需要人工标注的ImageNet数据集可简单多了。</p>\n<h4 id=\"Bert适用环境\"><a href=\"#Bert适用环境\" class=\"headerlink\" title=\"Bert适用环境\"></a>Bert适用环境</h4><p>第一，如果NLP任务偏向在语言本身中就包含答案，而不特别依赖文本外的其它特征，往往应用Bert能够极大提升应用效果。典型的任务比如QA和阅读理解，正确答案更偏向对语言的理解程度，理解能力越强，解决得越好，不太依赖语言之外的一些判断因素，所以效果提升就特别明显。反过来说，对于某些任务，除了文本类特征外，其它特征也很关键，比如搜索的用户行为／链接分析／内容质量等也非常重要，所以Bert的优势可能就不太容易发挥出来。再比如，推荐系统也是类似的道理，Bert可能只能对于文本内容编码有帮助，其它的用户行为类特征，不太容易融入Bert中。</p>\n<p>第二，Bert特别适合解决句子或者段落的匹配类任务。就是说，Bert特别适合用来解决判断句子关系类问题，这是相对单文本分类任务和序列标注等其它典型NLP任务来说的，很多实验结果表明了这一点。而其中的原因，我觉得很可能主要有两个，一个原因是：很可能是因为Bert在预训练阶段增加了Next Sentence Prediction任务，所以能够在预训练阶段学会一些句间关系的知识，而如果下游任务正好涉及到句间关系判断，就特别吻合Bert本身的长处，于是效果就特别明显。第二个可能的原因是：因为Self Attention机制自带句子A中单词和句子B中任意单词的Attention效果，而这种细粒度的匹配对于句子匹配类的任务尤其重要，所以Transformer的本质特性也决定了它特别适合解决这类任务。</p>\n<p>从上面这个Bert的擅长处理句间关系类任务的特性，我们可以继续推理出以下观点：</p>\n<p>既然预训练阶段增加了Next Sentence Prediction任务，就能对下游类似性质任务有较好促进作用，那么是否可以继续在预训练阶段加入其它的新的辅助任务？而这个辅助任务如果具备一定通用性，可能会对一类的下游任务效果有直接促进作用。这也是一个很有意思的探索方向，当然，这种方向因为要动Bert的第一个预训练阶段，所以属于NLP届土豪们的工作范畴，穷人们还是散退、旁观、鼓掌、叫好为妙。</p>\n<p>第三，Bert的适用场景，与NLP任务对深层语义特征的需求程度有关。感觉越是需要深层语义特征的任务，越适合利用Bert来解决；而对有些NLP任务来说，浅层的特征即可解决问题，典型的浅层特征性任务比如分词，POS词性标注，NER，文本分类等任务，这种类型的任务，只需要较短的上下文，以及浅层的非语义的特征，貌似就可以较好地解决问题，所以Bert能够发挥作用的余地就不太大，有点杀鸡用牛刀，有力使不出来的感觉。</p>\n<p>这很可能是因为Transformer层深比较深，所以可以逐层捕获不同层级不同深度的特征。于是，对于需要语义特征的问题和任务，Bert这种深度捕获各种特征的能力越容易发挥出来，而浅层的任务，比如分词／文本分类这种任务，也许传统方法就能解决得比较好，因为任务特性决定了，要解决好它，不太需要深层特征。</p>\n<p>第四，Bert比较适合解决输入长度不太长的NLP任务，而输入比较长的任务，典型的比如文档级别的任务，Bert解决起来可能就不太好。主要原因在于：Transformer的self attention机制因为要对任意两个单词做attention计算，所以时间复杂度是n平方，n是输入的长度。如果输入长度比较长，Transformer的训练和推理速度掉得比较厉害，于是，这点约束了Bert的输入长度不能太长。所以对于输入长一些的文档级别的任务，Bert就不容易解决好。结论是：Bert更适合解决句子级别或者段落级别的NLP任务。</p>\n<hr>\n<p>1、下载BERT中文模型 </p>\n<p>下载地址: <a href=\"https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip\">https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip</a></p>\n<p>2、把下载好的模型添加到当前目录下</p>\n<p>3、句向量生成</p>\n<p>生成句向量不需要做fine tune（微调），使用预先训练好的模型即可，可参考<code>extract_feature.py</code>的<code>main</code>方法，注意参数必须是一个list。</p>\n<p>首次生成句向量时需要加载graph，并在output_dir路径下生成一个新的graph文件，因此速度比较慢，再次调用速度会很快</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from bert.extrac_feature import BertVector</span><br><span class=\"line\">bv &#x3D; BertVector()</span><br><span class=\"line\">bv.encode([&#39;今天天气不错&#39;])</span><br></pre></td></tr></table></figure>\n\n<p>4、文本分类</p>\n<p>文本分类需要做fine tune，首先把数据准备好存放在<code>data</code>目录下，训练集的名字必须为<code>train.csv</code>，验证集的名字必须为<code>dev.csv</code>，测试集的名字必须为<code>test.csv</code>，<br>必须先调用<code>set_mode</code>方法，可参考<code>similarity.py</code>的<code>main</code>方法，</p>\n<p>训练：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from similarity import BertSim</span><br><span class=\"line\">import tensorflow as tf</span><br><span class=\"line\"></span><br><span class=\"line\">bs &#x3D; BertSim()</span><br><span class=\"line\">bs.set_mode(tf.estimator.ModeKeys.TRAIN)</span><br><span class=\"line\">bs.train()</span><br></pre></td></tr></table></figure>\n\n<p>验证：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from similarity import BertSim</span><br><span class=\"line\">import tensorflow as tf</span><br><span class=\"line\"></span><br><span class=\"line\">bs &#x3D; BertSim()</span><br><span class=\"line\">bs.set_mode(tf.estimator.ModeKeys.EVAL)</span><br><span class=\"line\">bs.eval()</span><br></pre></td></tr></table></figure>\n\n<p>测试：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from similarity import BertSim</span><br><span class=\"line\">import tensorflow as tf</span><br><span class=\"line\"></span><br><span class=\"line\">bs &#x3D; BertSim()</span><br><span class=\"line\">bs.set_mode(tf.estimator.ModeKeys.PREDICT)</span><br><span class=\"line\">bs.test()</span><br></pre></td></tr></table></figure>\n\n\n\n\n\n<h3 id=\"其他进行语句相似度计算的方法\"><a href=\"#其他进行语句相似度计算的方法\" class=\"headerlink\" title=\"其他进行语句相似度计算的方法\"></a>其他进行语句相似度计算的方法</h3><ul>\n<li><strong>编辑距离计算</strong></li>\n<li><strong>杰卡德系数计算</strong></li>\n<li><strong>TF 计算</strong></li>\n<li><strong>TFIDF 计算</strong></li>\n<li><strong>Word2Vec 计算</strong></li>\n</ul>\n<h3 id=\"BERT优点\"><a href=\"#BERT优点\" class=\"headerlink\" title=\"BERT优点\"></a><strong>BERT优点</strong></h3><ul>\n<li>Transformer Encoder因为有Self-attention机制，因此BERT自带双向功能</li>\n<li>因为双向功能以及多层Self-attention机制的影响，使得BERT必须使用Cloze版的语言模型Masked-LM来完成token级别的预训练</li>\n<li>为了获取比词更高级别的句子级别的语义表征，BERT加入了Next Sentence Prediction来和Masked-LM一起做联合训练</li>\n<li>为了适配多任务下的迁移学习，BERT设计了更通用的输入层和输出层</li>\n<li>微调成本小</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">- bert句向量、文本相似度</span><br><span class=\"line\">    - bert/extract_keras_bert_feature.py:提取bert句向量特征</span><br><span class=\"line\">    - bert/tet_bert_keras_sim.py:测试xlnet句向量cosin相似度</span><br><span class=\"line\">- xlnet句向量、文本相似度</span><br><span class=\"line\">    - xlnet/extract_keras_xlnet_feature.py:提取bert句向量特征</span><br><span class=\"line\">    - xlnet/tet_xlnet_keras_sim.py:测试bert句向量cosin相似度</span><br><span class=\"line\">- normalization_util指的是数据归一化</span><br><span class=\"line\">    - <span class=\"number\">0</span><span class=\"number\">-1</span>归一化处理</span><br><span class=\"line\">    - 均值归一化</span><br><span class=\"line\">    - sig归一化处理</span><br><span class=\"line\">- sim feature（ML）</span><br><span class=\"line\">    - distance_text_or_vec:各种计算文本、向量距离等</span><br><span class=\"line\">    - distance_vec_TS_SS：TS_SS计算词向量距离</span><br><span class=\"line\">    - cut_td_idf：将小黄鸡语料和gossip结合</span><br><span class=\"line\">    - sentence_sim_feature：计算两个文本的相似度或者距离，例如qq（问题和问题），或者qa（问题和答案）</span><br><span class=\"line\">    </span><br><span class=\"line\">    - 检索式ChatBot</span><br><span class=\"line\">    - 像ES那样直接检索(如使用fuzzywuzzy)，只能字面匹配</span><br><span class=\"line\">    - 构造句向量，检索问答库，能够检索有同义词的句子</span><br><span class=\"line\">    </span><br></pre></td></tr></table></figure>\n\n<h3 id=\"DATA来源\"><a href=\"#DATA来源\" class=\"headerlink\" title=\"DATA来源\"></a>DATA来源</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">- chinese_L-12_H-768_A-12（谷歌预训练好的模型）</span><br><span class=\"line\">   github项目中只是上传部分数据，需要的前往链接: https:&#x2F;&#x2F;pan.baidu.com&#x2F;s&#x2F;1I3vydhmFEQ9nuPG2fDou8Q 提取码: rket</span><br><span class=\"line\">   解压后就可以啦</span><br><span class=\"line\">- chinese_xlnet_mid_L-24_H-768_A-12(哈工大训练的中文xlnet, mid, 24层, wiki语料+通用语料)</span><br><span class=\"line\">    - 下载地址[https:&#x2F;&#x2F;github.com&#x2F;ymcui&#x2F;Chinese-PreTrained-XLNet](https:&#x2F;&#x2F;github.com&#x2F;ymcui&#x2F;Chinese-PreTrained-XLNet)</span><br><span class=\"line\">- chinese_vector</span><br><span class=\"line\">    github项目中只是上传部分数据，需要的前往链接: https:&#x2F;&#x2F;pan.baidu.com&#x2F;s&#x2F;1I3vydhmFEQ9nuPG2fDou8Q 提取码: rket</span><br><span class=\"line\">    - 截取的部分word2vec训练词向量（自己需要下载全效果才会好）</span><br><span class=\"line\">    - w2v_model_wiki_char.vec、w2v_model_wiki_word.vec都只有部分，词向量w2v_model_wiki_word.vec可以用这个下载地址的替换[https:&#x2F;&#x2F;pan.baidu.com&#x2F;s&#x2F;14JP1gD7hcmsWdSpTvA3vKA](https:&#x2F;&#x2F;pan.baidu.com&#x2F;s&#x2F;14JP1gD7hcmsWdSpTvA3vKA)</span><br><span class=\"line\"></span><br><span class=\"line\">- corpus</span><br><span class=\"line\">    github项目中只是上传部分数据，需要的前往链接: https:&#x2F;&#x2F;pan.baidu.com&#x2F;s&#x2F;1I3vydhmFEQ9nuPG2fDou8Q 提取码: rket</span><br><span class=\"line\">    - ner(train、dev、test----人民日报语料)</span><br><span class=\"line\">    - webank(train、dev、test)</span><br><span class=\"line\">    - 小黄鸡和gossip问答预料（数据没清洗）,chicken_and_gossip.txt</span><br><span class=\"line\">    - 微众银行和支付宝文本相似度竞赛数据， sim_webank.csv</span><br><span class=\"line\">- sentence_vec_encode_char</span><br><span class=\"line\">    - 1.txt（字向量生成的前100000句向量）</span><br><span class=\"line\">- sentence_vec_encode_word</span><br><span class=\"line\">    - 1.txt（词向量生成的前100000句向量）</span><br><span class=\"line\">- tf_idf（chicken_and_gossip.txt生成的tf-idf）</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"语句相似度计算方法\"><a href=\"#语句相似度计算方法\" class=\"headerlink\" title=\"语句相似度计算方法\"></a>语句相似度计算方法</h3><p>余弦值的计算</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cosθ&#x3D;a·b&#x2F;|a|*|b|</span><br><span class=\"line\">np.sum(query_bert_vec * ques_basic_vecs, axis&#x3D;1) &#x2F; np.linalg.norm(ques_basic_vecs, axis&#x3D;1)</span><br></pre></td></tr></table></figure>\n\n","categories":[{"name":"服务器","slug":"服务器","count":2,"path":"api/categories/服务器.json"}],"tags":[{"name":"-深度学习 - 自然语言处理,相似度计算","slug":"深度学习-自然语言处理-相似度计算","count":1,"path":"api/tags/深度学习-自然语言处理-相似度计算.json"}]}