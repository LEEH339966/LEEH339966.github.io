{"name":"-深度学习 - 自然语言处理,相似度计算","slug":"深度学习-自然语言处理-相似度计算","count":1,"postlist":[{"title":"bert-utils和其他语句相似度计算的方法","slug":"笔记语句相似度的计算方法","date":"2020-12-27T12:04:58.000Z","updated":"2020-12-27T13:31:50.939Z","comments":true,"path":"api/articles/笔记语句相似度的计算方法.json","excerpt":"<hr>\n<h2 id=\"bert-utils和其他语句相似度计算的方法\"><a href=\"#bert-utils和其他语句相似度计算的方法\" class=\"headerlink\" title=\"bert-utils和其他语句相似度计算的方法\"></a>bert-utils和其他语句相似度计算的方法</h2><h4 id=\"Bert模型的介绍\"><a href=\"#Bert模型的介绍\" class=\"headerlink\" title=\"Bert模型的介绍\"></a>Bert模型的介绍</h4><p>BERT的全称为Bidirectional Encoder Representation from Transformers，是一个预训练的语言表征模型。它强调了不再像以往一样采用传统的单向语言模型或者把两个单向语言模型进行浅层拼接的方法进行预训练，而是采用新的<strong>masked language model（MLM）</strong>，以致能生成<strong>深度的双向</strong>语言表征。BERT论文发表时提及在11个NLP（Natural Language Processing，自然语言处理）任务中获得了新的state-of-the-art的结果，令人目瞪口呆。</p>\n<p>该模型有以下主要优点：</p>\n<p>1）采用MLM对双向的Transformers进行预训练，以生成深层的双向语言表征。</p>\n<p>2）预训练后，只需要添加一个额外的输出层进行fine-tune，就可以在各种各样的下游任务中取得state-of-the-art的表现。在这过程中并不需要对BERT进行任务特定的结构修改。</p>","keywords":null,"cover":"https://leehbucket1.oss-cn-beijing.aliyuncs.com/img/9260af14410452b2bdf645b1cd80e632.jpg","content":null,"text":"bert-utils和其他语句相似度计算的方法Bert模型的介绍BERT的全称为Bidirectional Encoder Representation from Transformers，是一个预训练的语言表征模型。它强调了不再像以往一样采用传统的单向语言模型或者把两个单向语言","link":"","raw":null,"photos":[],"categories":[{"name":"服务器","slug":"服务器","count":2,"path":"api/categories/服务器.json"}],"tags":[{"name":"-深度学习 - 自然语言处理,相似度计算","slug":"深度学习-自然语言处理-相似度计算","count":1,"path":"api/tags/深度学习-自然语言处理-相似度计算.json"}]}]}