{"name":"服务器","slug":"服务器","count":2,"postlist":[{"title":"自己的服务器配置LT2P","slug":"自己的服务器配置LT2P","date":"2020-12-03T04:39:06.000Z","updated":"2020-12-03T04:45:15.568Z","comments":true,"path":"api/articles/自己的服务器配置LT2P.json","excerpt":"<hr>\n<h2 id=\"自己的服务器配置LT2P\"><a href=\"#自己的服务器配置LT2P\" class=\"headerlink\" title=\"自己的服务器配置LT2P\"></a>自己的服务器配置LT2P</h2><p>经过自己一天的研究终于搭建起了自己的第一个国外的服务器,本文介绍下自己的经验.<br>L2TP是一种工业标准的Internet隧道协议，功能大致和PPTP协议类似，比如同样可以对网络数据流进行加密。不过也有不同之处，比如PPTP要求网络为IP网络，L2TP要求面向数据包的点对点连接；PPTP使用单一隧道，L2TP使用多隧道；L2TP提供包头压缩、隧道验证，而PPTP不支持。</p>","keywords":null,"cover":"https://leehbucket1.oss-cn-beijing.aliyuncs.com/img/d06fa633d8b87eb900f576a06d74c3a5.jpg","content":null,"text":"自己的服务器配置LT2P经过自己一天的研究终于搭建起了自己的第一个国外的服务器,本文介绍下自己的经验.<br>L2TP是一种工业标准的Internet隧道协议，功能大致和PPTP协议类似，比如同样可以对网络数据流进行加密。不过也有不同之处，比如PPTP要求网络为IP网络，L2TP","link":"","raw":null,"photos":[],"categories":[{"name":"服务器","slug":"服务器","count":2,"path":"api/categories/服务器.json"}],"tags":[{"name":"-linux 服务器  -vpn","slug":"linux-服务器-vpn","count":1,"path":"api/tags/linux-服务器-vpn.json"}]},{"title":"bert-utils和其他语句相似度计算的方法","slug":"笔记语句相似度的计算方法","date":"2020-12-27T12:04:58.000Z","updated":"2020-12-27T13:31:50.939Z","comments":true,"path":"api/articles/笔记语句相似度的计算方法.json","excerpt":"<hr>\n<h2 id=\"bert-utils和其他语句相似度计算的方法\"><a href=\"#bert-utils和其他语句相似度计算的方法\" class=\"headerlink\" title=\"bert-utils和其他语句相似度计算的方法\"></a>bert-utils和其他语句相似度计算的方法</h2><h4 id=\"Bert模型的介绍\"><a href=\"#Bert模型的介绍\" class=\"headerlink\" title=\"Bert模型的介绍\"></a>Bert模型的介绍</h4><p>BERT的全称为Bidirectional Encoder Representation from Transformers，是一个预训练的语言表征模型。它强调了不再像以往一样采用传统的单向语言模型或者把两个单向语言模型进行浅层拼接的方法进行预训练，而是采用新的<strong>masked language model（MLM）</strong>，以致能生成<strong>深度的双向</strong>语言表征。BERT论文发表时提及在11个NLP（Natural Language Processing，自然语言处理）任务中获得了新的state-of-the-art的结果，令人目瞪口呆。</p>\n<p>该模型有以下主要优点：</p>\n<p>1）采用MLM对双向的Transformers进行预训练，以生成深层的双向语言表征。</p>\n<p>2）预训练后，只需要添加一个额外的输出层进行fine-tune，就可以在各种各样的下游任务中取得state-of-the-art的表现。在这过程中并不需要对BERT进行任务特定的结构修改。</p>","keywords":null,"cover":"https://leehbucket1.oss-cn-beijing.aliyuncs.com/img/9260af14410452b2bdf645b1cd80e632.jpg","content":null,"text":"bert-utils和其他语句相似度计算的方法Bert模型的介绍BERT的全称为Bidirectional Encoder Representation from Transformers，是一个预训练的语言表征模型。它强调了不再像以往一样采用传统的单向语言模型或者把两个单向语言","link":"","raw":null,"photos":[],"categories":[{"name":"服务器","slug":"服务器","count":2,"path":"api/categories/服务器.json"}],"tags":[{"name":"-深度学习 - 自然语言处理,相似度计算","slug":"深度学习-自然语言处理-相似度计算","count":1,"path":"api/tags/深度学习-自然语言处理-相似度计算.json"}]}]}